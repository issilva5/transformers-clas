[
  {
    "objectID": "transformers.html#arquitetura-sequence-to-sequence",
    "href": "transformers.html#arquitetura-sequence-to-sequence",
    "title": "Transformers",
    "section": "Arquitetura sequence-to-sequence",
    "text": "Arquitetura sequence-to-sequence\nUm modelo seq2seq recebe uma sequência de itens (palavras, letras, etc) e gera como saída outra sequência de itens."
  },
  {
    "objectID": "transformers.html#arquitetura-sequence-to-sequence-1",
    "href": "transformers.html#arquitetura-sequence-to-sequence-1",
    "title": "Transformers",
    "section": "Arquitetura sequence-to-sequence",
    "text": "Arquitetura sequence-to-sequence\nPodemos desmembrar um modelo seq2seq em duas partes: enconder e decoder.\n\nO contexto é um vetor que compila a informação da sequência de entrada."
  },
  {
    "objectID": "transformers.html#arquitetura-sequence-to-sequence-2",
    "href": "transformers.html#arquitetura-sequence-to-sequence-2",
    "title": "Transformers",
    "section": "Arquitetura sequence-to-sequence",
    "text": "Arquitetura sequence-to-sequence\nTipicamente esses modelos são implementados utilizando Redes Neurais Recorrentes."
  },
  {
    "objectID": "transformers.html#arquitetura-sequence-to-sequence-3",
    "href": "transformers.html#arquitetura-sequence-to-sequence-3",
    "title": "Transformers",
    "section": "Arquitetura sequence-to-sequence",
    "text": "Arquitetura sequence-to-sequence\n\n\nEntretanto, esse tipo de implementação tem um gargalo: toda a informação da sequência de entrada fica comprimida na última camada.\nIsso tornou difícil para estes modelos lidarem com longas sequências, pois o real contexto das palavras se perdia.\nPara mitigar isso foi proposta uma solução: Attention."
  },
  {
    "objectID": "transformers.html#seq2seq-com-attention",
    "href": "transformers.html#seq2seq-com-attention",
    "title": "Transformers",
    "section": "Seq2Seq com Attention",
    "text": "Seq2Seq com Attention\nAo invés de passar a última camada (hidden state) como contexto, o encoder ‘passa’ todas as camadas para o decoder."
  },
  {
    "objectID": "transformers.html#seq2seq-com-attention-1",
    "href": "transformers.html#seq2seq-com-attention-1",
    "title": "Transformers",
    "section": "Seq2Seq com Attention",
    "text": "Seq2Seq com Attention\nPor sua vez, o decoder realiza um passo extra antes de gerar sua saída. Para conseguir focar nas partes da entrada que são relevantes no momento atual, o decoder faz o seguinte:\n\n\nCalcula um peso para cada hidden state que recebeu do enconder;\nAplica a função softmax para transformar os pesos numa distribuição de probabilidade;\nRealiza uma soma ponderada entre cada hidden state e seu peso transformado;\nConcatena o hidden state atual com o novo vetor de contexto."
  },
  {
    "objectID": "transformers.html#seq2seq-com-attention-2",
    "href": "transformers.html#seq2seq-com-attention-2",
    "title": "Transformers",
    "section": "Seq2Seq com Attention",
    "text": "Seq2Seq com Attention"
  },
  {
    "objectID": "transformers.html#o-problema-das-redes-recorrentes-1",
    "href": "transformers.html#o-problema-das-redes-recorrentes-1",
    "title": "Transformers",
    "section": "O problema das redes recorrentes",
    "text": "O problema das redes recorrentes\nApesar de terem obtidos bons resultados, as redes recorrentes sofrem de dois problemas:\n\n\n\n\nDificuldade de paralelização\n\n\n\n\n\n\nDistância de interação linear"
  },
  {
    "objectID": "transformers.html#surgem-os-transformers",
    "href": "transformers.html#surgem-os-transformers",
    "title": "Transformers",
    "section": "Surgem os transformers",
    "text": "Surgem os transformers\nPara tentar resolver os problemas anteriores, um novo modelo foi proposto em um artigo chamado Attention is All You Need: os Transformers."
  },
  {
    "objectID": "transformers.html#vamos-entender-melhor-esses-modelos",
    "href": "transformers.html#vamos-entender-melhor-esses-modelos",
    "title": "Transformers",
    "section": "Vamos entender melhor esses modelos",
    "text": "Vamos entender melhor esses modelos"
  },
  {
    "objectID": "transformers.html#vamos-entender-melhor-esses-modelos-1",
    "href": "transformers.html#vamos-entender-melhor-esses-modelos-1",
    "title": "Transformers",
    "section": "Vamos entender melhor esses modelos",
    "text": "Vamos entender melhor esses modelos"
  },
  {
    "objectID": "transformers.html#vamos-entender-melhor-esses-modelos-2",
    "href": "transformers.html#vamos-entender-melhor-esses-modelos-2",
    "title": "Transformers",
    "section": "Vamos entender melhor esses modelos",
    "text": "Vamos entender melhor esses modelos"
  },
  {
    "objectID": "transformers.html#vamos-entender-melhor-esses-modelos-3",
    "href": "transformers.html#vamos-entender-melhor-esses-modelos-3",
    "title": "Transformers",
    "section": "Vamos entender melhor esses modelos",
    "text": "Vamos entender melhor esses modelos"
  },
  {
    "objectID": "transformers.html#self-attention",
    "href": "transformers.html#self-attention",
    "title": "Transformers",
    "section": "Self-attention",
    "text": "Self-attention\nPodemos imaginar attention como sendo uma tabela de consulta (dicionário) com um acesso um pouco diferente do convencional.\n\n\n\n\nTabela de consulta tradicional\n\n\n\n\n\n\nAttention\n\n\n\nA consulta dá match com todas as chaves, de acordo com pesos entre 0 e 1. Os valores são então multiplicados pelos pesos e somados."
  },
  {
    "objectID": "transformers.html#self-attention-1",
    "href": "transformers.html#self-attention-1",
    "title": "Transformers",
    "section": "Self-attention",
    "text": "Self-attention\nA camada de self-attention irá criar uma representação contextual para cada palavra da entrada.\n\n\n\n\n\n\n\n\n\n\nO número de operações impossíveis de paralelizar não cresce com o tamanho da entrada.\nDistância de interação máxima: O(1), dado que todas as palavras interagem em todas as camadas."
  },
  {
    "objectID": "transformers.html#como-calcular",
    "href": "transformers.html#como-calcular",
    "title": "Transformers",
    "section": "Como calcular?",
    "text": "Como calcular?\nVamos precisar de algumas matrizes \\(W^Q\\), \\(W^K\\) e \\(W^V\\). Essas matrizes iram gerar três representações para cada palavra: chave (key), valor (value) e consulta (query)."
  },
  {
    "objectID": "transformers.html#como-calcular-1",
    "href": "transformers.html#como-calcular-1",
    "title": "Transformers",
    "section": "Como calcular?",
    "text": "Como calcular?\nA camada de self-attention irá criar uma representação contextual para cada palavra da entrada."
  },
  {
    "objectID": "transformers.html#multi-head-self-attention",
    "href": "transformers.html#multi-head-self-attention",
    "title": "Transformers",
    "section": "Multi-head self-attention",
    "text": "Multi-head self-attention"
  },
  {
    "objectID": "transformers.html#multi-head-self-attention-1",
    "href": "transformers.html#multi-head-self-attention-1",
    "title": "Transformers",
    "section": "Multi-head self-attention",
    "text": "Multi-head self-attention"
  },
  {
    "objectID": "transformers.html#codificando-a-posição",
    "href": "transformers.html#codificando-a-posição",
    "title": "Transformers",
    "section": "Codificando a posição",
    "text": "Codificando a posição"
  },
  {
    "objectID": "transformers.html#conexões-residuais",
    "href": "transformers.html#conexões-residuais",
    "title": "Transformers",
    "section": "Conexões residuais",
    "text": "Conexões residuais"
  },
  {
    "objectID": "transformers.html#camada-de-normalização",
    "href": "transformers.html#camada-de-normalização",
    "title": "Transformers",
    "section": "Camada de normalização",
    "text": "Camada de normalização"
  },
  {
    "objectID": "transformers.html#focando-no-enconder",
    "href": "transformers.html#focando-no-enconder",
    "title": "Transformers",
    "section": "Focando no enconder",
    "text": "Focando no enconder"
  },
  {
    "objectID": "transformers.html#e-o-decodificador",
    "href": "transformers.html#e-o-decodificador",
    "title": "Transformers",
    "section": "E o decodificador?",
    "text": "E o decodificador?"
  }
]