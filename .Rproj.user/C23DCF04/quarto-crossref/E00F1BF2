{"headings":["recapitulando","arquitetura-sequence-to-sequence","arquitetura-sequence-to-sequence-1","arquitetura-sequence-to-sequence-2","arquitetura-sequence-to-sequence-3","seq2seq-com-attention","seq2seq-com-attention-1","seq2seq-com-attention-2","o-problema-das-redes-recorrentes","o-problema-das-redes-recorrentes-1","attention-is-all-you-need","surgem-os-transformers","vamos-entender-melhor-esses-modelos","vamos-entender-melhor-esses-modelos-1","vamos-entender-melhor-esses-modelos-2","vamos-entender-melhor-esses-modelos-3","self-attetion","self-attention","self-attention-1","como-calcular","como-calcular-1","multi-head-self-attention","multi-head-self-attention-1","algumas-melhorias","codificando-a-posição","conexões-residuais","camada-de-normalização","retomando-nosso-modelo","focando-no-enconder","e-o-decodificador"],"entries":[]}