---
title: "Transformers"
author: "Ítallo Silva"
format: 
  revealjs:
    auto-play-media: true
---

# Recapitulando

## Arquitetura sequence-to-sequence

Um modelo seq2seq recebe uma sequência de itens (palavras, letras, etc) e gera como saída outra sequência de itens.

::: {.fragment}
<video src="media/seq2seq_1.mp4" loop/>
:::

## Arquitetura sequence-to-sequence

Podemos desmembrar um modelo seq2seq em duas partes: **enconder** e **decoder**.

::: {.fragment}
O <span style="color:#F39019;">contexto</span> é um vetor que compila a informação da sequência de entrada.
:::

<video src="media/seq2seq_3.mp4" loop/>

## Arquitetura sequence-to-sequence

Tipicamente esses modelos são implementados utilizando Redes Neurais Recorrentes.

::: {.fragment}
<video src="media/seq2seq_6.mp4" loop/>
:::

## Arquitetura sequence-to-sequence

::: {.incremental}

- Entretanto, esse tipo de implementação tem um gargalo: **toda a informação da sequência de entrada fica comprimida na última camada**.

- Isso tornou difícil para estes modelos lidarem com longas sequências, pois o real contexto das palavras se perdia.

- Para mitigar isso foi proposta uma solução: **Attention**.

:::

## Seq2Seq com Attention

Ao invés de passar a última camada (<span style="color:#F39019;">hidden state</span>) como contexto, o encoder 'passa' todas as camadas para o decoder.

<video src="media/seq2seq_7.mp4" loop/>

## Seq2Seq com Attention {.smaller}

Por sua vez, o decoder realiza um passo extra antes de gerar sua saída. Para conseguir focar nas partes da entrada que são relevantes no momento atual, o decoder faz o seguinte:

::: {.incremental}

- Calcula um peso para cada <span style="color:#F39019;">hidden state</span> que recebeu do enconder;
- Aplica a função softmax para transformar os pesos numa distribuição de probabilidade;
- Realiza uma soma ponderada entre cada <span style="color:#F39019;">hidden state</span> e seu peso transformado;
- Concatena o <span style="color:#B36AE2;">hidden state</span> atual com o novo vetor de contexto.

:::

## Seq2Seq com Attention

<video src="media/attention_process.mp4"/>

# O problema das redes recorrentes

## O problema das redes recorrentes
Apesar de terem obtidos bons resultados, as redes recorrentes sofrem de dois problemas:

::: columns
::: {.column width="45%"}
::: fragment
::: {style="text-align: center"}
*Dificuldade de paralelização*

![](media/parallelization.png)

:::
:::
:::

::: {.column width="45%"}
::: fragment
::: {style="text-align: center"}
*Distância de interação linear*

![](media/distance.png)
:::
:::
:::

:::

# Attention is All You Need

## Surgem os transformers

Para tentar resolver os problemas anteriores, um novo modelo foi proposto em um artigo chamado *Attention is All You Need*: os **Transformers**.

::: fragment

![](media/optimus_prime.png){fig-align="right"}

:::

## Vamos entender melhor esses modelos

![](https://jalammar.github.io/images/t/the_transformer_3.png){fig-align="center"}

## Vamos entender melhor esses modelos

![](https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png){fig-align="center"}

## Vamos entender melhor esses modelos

![](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png){fig-align="center"}

## Vamos entender melhor esses modelos

![](https://jalammar.github.io/images/t/Transformer_decoder.png){fig-align="center"}

# Self-attetion

## Self-attention {.smaller}

Podemos imaginar attention como sendo uma tabela de consulta (dicionário) com um acesso um pouco diferente do convencional.

::: columns
::: {.column width="45%"}
::: fragment
::: {style="text-align: center"}
*Tabela de consulta tradicional*

![](media/lookup_simple.png)

:::
:::
:::

::: {.column width="45%"}
::: fragment
::: {style="text-align: center"}
*Attention*

![](media/lookup_attention.png)

:::

::: fragment
A consulta dá *match* com todas as chaves, de acordo com pesos entre 0 e 1. Os valores são então multiplicados pelos pesos e somados.
:::

:::
:::

:::


## Self-attention {.smaller}

A camada de self-attention irá criar uma representação contextual para cada palavra da entrada.

::: columns
::: {.column width="45%"}

![](media/self_attention.png){fig-align="center"}

:::

::: {.column width="45%"}
::: {.incremental}

- O número de operações impossíveis de paralelizar não cresce com o tamanho da entrada.
- Distância de interação máxima: O(1), dado que todas as palavras interagem em todas as camadas.

:::
:::

:::


## Como calcular?

Vamos precisar de algumas matrizes $W^Q$, $W^K$ e $W^V$. Essas matrizes iram gerar três representações para cada palavra: chave (key), valor (value) e consulta (query).

![](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png){fig-align="center"}

## Como calcular? {.smaller}

A camada de self-attention irá criar uma representação contextual para cada palavra da entrada.

![](https://jalammar.github.io/images/t/self-attention-output.png){fig-align="center"}

## Matematicamente... {.smaller}

Considere $w_{1:n}$ como uma sequência de palavras em um vocabulário $V$, por exemplo, *Aqui jaz Dobby, um elfo livre*. Para cada $w_i$:

::: {.incremental}

1. Seja $x_i = Ew_i$, onde $E \in \mathbb{R}^{d\times|V|}$ é uma matriz de embeddings.
2. Transforme cada *word embedding* com as matrizes $W^Q, W^K, W^V \in \mathbb{R}^{d\times d}$.
$$q_i = W^Qx_i \textrm{ (consultas)} \qquad k_i = W^Kx_i \textrm{ (chaves)} \qquad v_i = W^Vx_i \textrm{ (valores)}$$
3. Calcule os scores entre consultas e chaves e normalize com softmax:
$$e_{ij} = \frac{q_i^Tk_j}{\sqrt{d}} \qquad\qquad \alpha_{ij} = \frac{exp(e_{ij})}{\sum_{j'=1}^n exp(e_{ij'})}$$
5. Calcule o valor de saída:
$$z_{i} = \sum_{j=1}^n \alpha_{ij}v_j$$

:::

# Algumas melhorias

## Codificando a posição {.smaller}

Na forma atual como construímos nossa camada de attention, a ordem das palavras no documento não importa. Por exemplo, considere as seguintes frases:

- Só João alugou a casa de praia;
- João só alugou a casa de praia;
- João alugou só a casa de praia.

::: {.fragment}

A palavra **só** tem diferentes significados que dependem da sua posição. Com as fórmulas que definimos como ficariam os pesos $\alpha_{só,João}$ em cada frase?

:::

## Codificando a posição

::: {.incremental}
- Ou seja, não existe depedência de posição na operação de *self-attetion*. 

- Porém, sabemos que a posição das palavras é importante para o sentido da frase, assim precisamos de uma maneira de inserir essa posição no cálculo.

- Uma forma simples de fazer isso é considerar uma matriz $P \in \mathbb{R}^{N\times d}$, em que N é o tamanho máximo da sentença que iremos considerar no modelo. E então mudar nosso $x_i$ por $\tilde{x_i} = x_i + p_i$.

:::

## Multi-head self-attention {.smaller}

Outro refinamento que pode ser aplicado à *self-attention*, é o que chamamos de *Multi-head self-attention*.

::: {.fragment}

![](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)

:::

# Retomando nosso modelo

![](https://jalammar.github.io/images/t/Transformer_decoder.png){fig-align="center"}

## Focando no enconder

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png){fig-align="center"}

## Focando no enconder

::: {.incremental}

- Conexões residuais
- Camada de normalização

:::

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png){fig-align="center"}

## E o decodificador?

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png){fig-align="center"}

## E o decodificador?

![](https://jalammar.github.io/images/t/transformer_decoding_1.gif)

## E o decodificador?

![](https://jalammar.github.io/images/t/transformer_decoding_2.gif)

## Referências {.smaller}

Alammar, J. The Illustrated Transformer, Jay Alammar, accessado em 26 de setembro de 2023, <https://jalammar.github.io/illustrated-transformer/>.

Alammar, J. Visualizing A Neural Machine Translation Model, Jay Alammar, accessado em 26 de setembro de 2023, <https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/>.

Hewitt, J. Note 10: Self-Attention & Transformers, CS 224n: Natural Language Processing with Deep Learning, acessado em 26 de setembro de 2023, <http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf>.
